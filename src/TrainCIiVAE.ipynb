{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dc2a94-47c0-4309-8a8d-8ee4e8a40b41",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b55da-7924-4f7e-8108-b25b56a8e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8ab51-c3bc-4fe0-96cc-477e3be838b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path to reflect your env\n",
    "%cd / content/gdrive/MyDrive/final-ml/ML-Final-Project\n",
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c77ea2-a913-4086-91ef-7b62b8f47fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/cs394n_project/CS394N/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98441a9e-e0a0-494b-a166-eeed278fd2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.nets import *\n",
    "from utils.model_tools import *\n",
    "from utils.ciivae_etc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309ba2fd-0cc2-4aa3-8054-80635f170c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "seed = 420\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc071b39-07f5-458a-82a7-9511e52e63bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e289053-ddfe-47a3-a43c-7a5fd94d7792",
   "metadata": {},
   "source": [
    "## Import data/Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d54f7e-5f36-4a0e-86e3-88a2e66350f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce0e35d4-3e05-48f8-92f4-c5fef359fec9",
   "metadata": {},
   "source": [
    "## Train CI-iVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758ea7bf-379a-415d-9118-0bd58d0fa52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 1 #TODO\n",
    "dim_u = 1 #TODO\n",
    "dim_z = 16\n",
    "hid_dim = 1 #TODO\n",
    "\n",
    "num_es = 100\n",
    "batch_size = 256\n",
    "num_workers = 32\n",
    "\n",
    "beta = 0.01\n",
    "weight_decay = 5e-6\n",
    "init_lr = 5e-5\n",
    "lr_milestones = [25, 50, 75]\n",
    "lr_gamma = 0.5\n",
    "\n",
    "dtype = torch.float32\n",
    "M = 50\n",
    "alpha_step = 0.025\n",
    "fix_alpha = None\n",
    "\n",
    "adam_betas = (0.5, 0.999)\n",
    "\n",
    "prior, encoder, decoder = ConvCIiVAE(dim_x, dim_u, hid_dim, dim_z=16, prior_node_list=[128, 128],\n",
    "            decoder_node_list=[4096, 4096],\n",
    "            decoder_final_activation='sigmoid')\n",
    "\n",
    "enc_optimizer = torch.optim.Adam(encoder.parameters(),\n",
    "                                     betas=adam_betas,\n",
    "                                     lr=init_lr,\n",
    "                                     weight_decay=weight_decay)\n",
    "gen_optimizer = torch.optim.Adam(list(decoder.parameters())\n",
    "                                     + list(prior.parameters()),\n",
    "                                     betas=adam_betas,\n",
    "                                     lr=init_lr,\n",
    "                                     weight_decay=weight_decay)\n",
    "\n",
    "enc_scheduler = torch.optim.lr_scheduler.MultiStepLR(enc_optimizer,\n",
    "                                                         milestones=lr_milestones,\n",
    "                                                         gamma=lr_gamma)\n",
    "gen_scheduler = torch.optim.lr_scheduler.MultiStepLR(gen_optimizer,\n",
    "                                                         milestones=lr_milestones,\n",
    "                                                         gamma=lr_gamma)\n",
    "\n",
    "mse_criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03ad0263-3053-4326-a869-7d4e0e79029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training log    \n",
    "loss_names = ['loss', 'recon_loss_post', 'kl_post_prior',\n",
    "                  'recon_loss_encoded', 'kl_encoded_prior', 'l2_penalty']\n",
    "logs = {}\n",
    "for datasetname in ['train', 'val']:\n",
    "    logs[datasetname] = {}\n",
    "    for loss_name in loss_names:\n",
    "        logs[datasetname][loss_name] = []\n",
    "summary_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8238a713-3682-4ccd-8f20-0a30bf816382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_batch, u_batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[43mdataloader\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m                                          desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] Training\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, num_epoch)):\n\u001b[1;32m     10\u001b[0m         prior\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m         encoder\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "\n",
    "# TODO: training progress\n",
    "\n",
    "for e in range(1, num_epochs + 1):\n",
    "    print(f\"Epoch {e}\\n-------------------------------\")\n",
    "    \n",
    "    # get dataloader setup\n",
    "    for x_batch, u_batch in tqdm.tqdm(dataloader['train'],\n",
    "                                         desc='[Epoch %d/%d] Training' % (epoch, num_epoch)):\n",
    "        prior.train()\n",
    "        encoder.train()\n",
    "        decoder.train() \n",
    "\n",
    "        enc_optimizer.zero_grad()\n",
    "        gen_optimizer.zero_grad()\n",
    "\n",
    "        # forward step\n",
    "        lam_mean, lam_log_var = prior(u_batch)\n",
    "        z_mean, z_log_var = encoder(x_batch)\n",
    "        post_mean, post_log_var = compute_posterior(z_mean, z_log_var, lam_mean, lam_log_var)\n",
    "        post_sample = sampling(post_mean, post_log_var)\n",
    "        encoded_sample = sampling(z_mean, z_log_var)\n",
    "\n",
    "        epsilon = torch.randn((z_mean.shape[0], z_mean.shape[1], M))\n",
    "        if device == 'cuda':\n",
    "            post_sample = post_sample.cuda()\n",
    "            encoded_sample = encoded_sample.cuda()\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        fire_rate_post, obs_log_var = decoder(post_sample)\n",
    "        fire_rate_encoded, _ = decoder(encoded_sample)\n",
    "\n",
    "        # compute objective function\n",
    "        obs_loglik_post = -torch.mean((fire_rate_post - x_batch)**2, dim=1)\n",
    "        obs_loglik_encoded = -torch.mean((fire_rate_encoded - x_batch)**2, dim=1)\n",
    "\n",
    "        kl_post_prior = kl_criterion(post_mean, post_log_var, lam_mean, lam_log_var)\n",
    "        kl_encoded_prior = kl_criterion(z_mean, z_log_var, lam_mean, lam_log_var)\n",
    "\n",
    "        elbo_post = obs_loglik_post - beta_kl_post_prior*kl_post_prior\n",
    "        elbo_encoded = obs_loglik_encoded - beta_kl_encoded_prior*kl_encoded_prior\n",
    "\n",
    "        z_mean_tiled = torch.tile(torch.unsqueeze(z_mean, 2), [1, 1, M])\n",
    "        z_log_var_tiled = torch.tile(torch.unsqueeze(z_log_var, 2), [1, 1, M])\n",
    "        z_sample_tiled = z_mean_tiled + torch.exp(0.5 * z_log_var_tiled) * epsilon\n",
    "\n",
    "        post_mean_tiled = torch.tile(torch.unsqueeze(post_mean, 2), [1, 1, M])\n",
    "        post_log_var_tiled = torch.tile(torch.unsqueeze(post_log_var, 2), [1, 1, M])\n",
    "        post_sample_tiled = post_mean_tiled + torch.exp(0.5 * post_log_var_tiled) * epsilon\n",
    "\n",
    "        log_z_density_with_post_sample = -torch.sum((post_sample_tiled - z_mean_tiled)**2/(2*torch.exp(z_log_var_tiled))+(z_log_var_tiled/2), dim=1)\n",
    "        log_post_density_with_post_sample = -torch.sum((post_sample_tiled - post_mean_tiled)**2/(2*torch.exp(post_log_var_tiled))+(post_log_var_tiled/2), dim=1)\n",
    "        log_z_density_with_z_sample = -torch.sum((z_sample_tiled - z_mean_tiled)**2/(2*torch.exp(z_log_var_tiled))+(z_log_var_tiled/2), dim=1)\n",
    "        log_post_density_with_z_sample = -torch.sum((z_sample_tiled - post_mean_tiled)**2/(2*torch.exp(post_log_var_tiled))+(post_log_var_tiled/2), dim=1)\n",
    "\n",
    "        if fix_alpha is not None:\n",
    "            if fix_alpha == 0.0:\n",
    "                loss = torch.mean(-elbo_post)\n",
    "            elif fix_alpha == 1.0:\n",
    "                loss = torch.mean(-elbo_encoded)\n",
    "            else:\n",
    "                ratio_z_over_post_with_post_sample = torch.exp(log_z_density_with_post_sample-log_post_density_with_post_sample)\n",
    "                ratio_post_over_z_with_z_sample = torch.exp(log_post_density_with_z_sample-log_z_density_with_z_sample)\n",
    "                skew_kl_post = torch.log(1.0/(fix_alpha*ratio_z_over_post_with_post_sample+(1.0-fix_alpha)))\n",
    "                skew_kl_post = torch.abs(torch.mean(skew_kl_post, dim=-1))\n",
    "                skew_kl_encoded = torch.log(1.0/(fix_alpha+(1.0-fix_alpha)*ratio_post_over_z_with_z_sample))\n",
    "                skew_kl_encoded = torch.abs(torch.mean(skew_kl_encoded, dim=-1))\n",
    "                loss = -fix_alpha*elbo_encoded-(1.0-fix_alpha)*elbo_post+fix_alpha*skew_kl_encoded+(1.0-fix_alpha)*skew_kl_post\n",
    "        else:\n",
    "            alpha_list = np.arange(alpha_step, 1.0, alpha_step)\n",
    "            loss = torch.zeros((elbo_post.shape[0], len(alpha_list)))\n",
    "            i = 0\n",
    "            for alpha in alpha_list:\n",
    "                ratio_z_over_post_with_post_sample = torch.exp(log_z_density_with_post_sample-log_post_density_with_post_sample)\n",
    "                ratio_post_over_z_with_z_sample = torch.exp(log_post_density_with_z_sample-log_z_density_with_z_sample)\n",
    "                skew_kl_post = torch.log(1.0/(alpha*ratio_z_over_post_with_post_sample+(1.0-alpha)))\n",
    "                skew_kl_post = torch.abs(torch.mean(skew_kl_post, dim=-1))\n",
    "                skew_kl_encoded = torch.log(1.0/(alpha+(1.0-alpha)*ratio_post_over_z_with_z_sample))\n",
    "                skew_kl_encoded = torch.abs(torch.mean(skew_kl_encoded, dim=-1))\n",
    "                loss[:, i] = -alpha*elbo_encoded-(1.0-alpha)*elbo_post+alpha*skew_kl_encoded+(1.0-alpha)*skew_kl_post\n",
    "                i += 1\n",
    "            loss, _ = torch.min(loss, dim = 1)\n",
    "\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        enc_optimizer.step()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "\n",
    "    prior.eval()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for datasetname in ['train', 'val']:\n",
    "        loss_cumsum, sample_size = 0.0, 0\n",
    "        obs_loglik_post_cumsum, kl_post_prior_cumsum = 0.0, 0.0\n",
    "        obs_loglik_encoded_cumsum, kl_encoded_prior_cumsum = 0.0, 0.0\n",
    "        for x_batch, u_batch in tqdm.tqdm(dataloader[datasetname],\n",
    "                                         desc='[Epoch %d/%d] Computing loss terms on %s' % (e, num_e, datasetname)):\n",
    "            if device == 'cuda':\n",
    "                x_batch, u_batch = x_batch.cuda(), u_batch.cuda()\n",
    "\n",
    "            # forward step\n",
    "            lam_mean, lam_log_var = prior(u_batch)\n",
    "            z_mean, z_log_var = encoder(x_batch)\n",
    "            post_mean, post_log_var = compute_posterior(z_mean, z_log_var, lam_mean, lam_log_var)\n",
    "            post_sample = sampling(post_mean, post_log_var)\n",
    "            encoded_sample = sampling(z_mean, z_log_var)\n",
    "\n",
    "            epsilon = torch.randn((z_mean.shape[0], z_mean.shape[1], M))\n",
    "            if device == 'cuda':\n",
    "                post_sample = post_sample.cuda()\n",
    "                encoded_sample = encoded_sample.cuda()\n",
    "                epsilon = epsilon.cuda()\n",
    "\n",
    "            fire_rate_post, obs_log_var = decoder(post_sample)\n",
    "            fire_rate_encoded, _ = decoder(encoded_sample)\n",
    "\n",
    "            # compute objective function\n",
    "            obs_loglik_post = -torch.mean((fire_rate_post - x_batch)**2, dim=1)\n",
    "            obs_loglik_encoded = -torch.mean((fire_rate_encoded - x_batch)**2, dim=1)\n",
    "\n",
    "            kl_post_prior = kl_criterion(post_mean, post_log_var, lam_mean, lam_log_var)\n",
    "            kl_encoded_prior = kl_criterion(z_mean, z_log_var, lam_mean, lam_log_var)\n",
    "\n",
    "            elbo_pi_vae = obs_loglik_post - beta_kl_post_prior*kl_post_prior\n",
    "            elbo_vae = obs_loglik_encoded - beta_kl_encoded_prior*kl_encoded_prior\n",
    "\n",
    "            z_mean_tiled = torch.tile(torch.unsqueeze(z_mean, 2), [1, 1, M])\n",
    "            z_log_var_tiled = torch.tile(torch.unsqueeze(z_log_var, 2), [1, 1, M])\n",
    "            z_sample_tiled = z_mean_tiled + torch.exp(0.5 * z_log_var_tiled) * epsilon\n",
    "\n",
    "            post_mean_tiled = torch.tile(torch.unsqueeze(post_mean, 2), [1, 1, M])\n",
    "            post_log_var_tiled = torch.tile(torch.unsqueeze(post_log_var, 2), [1, 1, M])\n",
    "            post_sample_tiled = post_mean_tiled + torch.exp(0.5 * post_log_var_tiled) * epsilon\n",
    "\n",
    "            log_z_density_with_post_sample = -torch.sum((post_sample_tiled - z_mean_tiled)**2/(2*torch.exp(z_log_var_tiled))+(z_log_var_tiled/2), dim=1)\n",
    "            log_post_density_with_post_sample = -torch.sum((post_sample_tiled - post_mean_tiled)**2/(2*torch.exp(post_log_var_tiled))+(post_log_var_tiled/2), dim=1)\n",
    "            log_z_density_with_z_sample = -torch.sum((z_sample_tiled - z_mean_tiled)**2/(2*torch.exp(z_log_var_tiled))+(z_log_var_tiled/2), dim=1)\n",
    "            log_post_density_with_z_sample = -torch.sum((z_sample_tiled - post_mean_tiled)**2/(2*torch.exp(post_log_var_tiled))+(post_log_var_tiled/2), dim=1)\n",
    "\n",
    "            if fix_alpha is not None:\n",
    "                if fix_alpha == 0.0:\n",
    "                    loss = torch.mean(-elbo_post)\n",
    "                elif fix_alpha == 1.0:\n",
    "                    loss = torch.mean(-elbo_encoded)\n",
    "                else:\n",
    "                    ratio_z_over_post_with_post_sample = torch.exp(log_z_density_with_post_sample-log_post_density_with_post_sample)\n",
    "                    ratio_post_over_z_with_z_sample = torch.exp(log_post_density_with_z_sample-log_z_density_with_z_sample)\n",
    "                    skew_kl_post = torch.log(1.0/(fix_alpha*ratio_z_over_post_with_post_sample+(1.0-fix_alpha)))\n",
    "                    skew_kl_post = torch.abs(torch.mean(skew_kl_post, dim=-1))\n",
    "                    skew_kl_encoded = torch.log(1.0/(fix_alpha+(1.0-fix_alpha)*ratio_post_over_z_with_z_sample))\n",
    "                    skew_kl_encoded = torch.abs(torch.mean(skew_kl_encoded, dim=-1))\n",
    "                    loss = -fix_alpha*elbo_encoded-(1.0-fix_alpha)*elbo_post+fix_alpha*skew_kl_encoded+(1.0-fix_alpha)*skew_kl_post\n",
    "            else:\n",
    "                alpha_list = np.arange(alpha_step, 1.0, alpha_step)\n",
    "                loss = torch.zeros((elbo_post.shape[0], len(alpha_list)))\n",
    "                i = 0\n",
    "                for alpha in alpha_list:\n",
    "                    ratio_z_over_post_with_post_sample = torch.exp(log_z_density_with_post_sample-log_post_density_with_post_sample)\n",
    "                    ratio_post_over_z_with_z_sample = torch.exp(log_post_density_with_z_sample-log_z_density_with_z_sample)\n",
    "                    skew_kl_post = torch.log(1.0/(alpha*ratio_z_over_post_with_post_sample+(1.0-alpha)))\n",
    "                    skew_kl_post = torch.abs(torch.mean(skew_kl_post, dim=-1))\n",
    "                    skew_kl_encoded = torch.log(1.0/(alpha+(1.0-alpha)*ratio_post_over_z_with_z_sample))\n",
    "                    skew_kl_encoded = torch.abs(torch.mean(skew_kl_encoded, dim=-1))\n",
    "                    loss[:, i] = -alpha*elbo_encoded-(1.0-alpha)*elbo_post+alpha*skew_kl_encoded+(1.0-alpha)*skew_kl_post\n",
    "                    i += 1\n",
    "                loss, _ = torch.min(loss, dim = 1)\n",
    "            loss = torch.mean(loss)\n",
    "                \n",
    "            loss_cumsum += loss.item()*np.shape(x_batch)[0]\n",
    "            obs_loglik_post_cumsum += torch.mean(obs_loglik_post).item()*np.shape(x_batch)[0]\n",
    "            kl_post_prior_cumsum += torch.mean(kl_post_prior).item()*np.shape(x_batch)[0]\n",
    "            obs_loglik_encoded_cumsum += torch.mean(obs_loglik_encoded).item()*np.shape(x_batch)[0]\n",
    "            kl_encoded_prior_cumsum += torch.mean(kl_encoded_prior).item()*np.shape(x_batch)[0]\n",
    "            sample_size += np.shape(x_batch)[0]\n",
    "\n",
    "        l2_penalty = 0.0\n",
    "        for networks in [prior, encoder, decoder]:\n",
    "            for name, m in networks.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    l2_penalty += 0.5*torch.sum(m**2)\n",
    "        logs[datasetname]['loss'].append(loss_cumsum/sample_size)\n",
    "        logs[datasetname]['recon_loss_post'].append(-obs_loglik_post_cumsum/sample_size)\n",
    "        logs[datasetname]['kl_post_prior'].append(kl_post_prior_cumsum/sample_size)\n",
    "        logs[datasetname]['recon_loss_encoded'].append(-obs_loglik_encoded_cumsum/sample_size)\n",
    "        logs[datasetname]['kl_encoded_prior'].append(kl_encoded_prior_cumsum/sample_size)\n",
    "        logs[datasetname]['l2_penalty'].append(l2_penalty.item())\n",
    "        \n",
    "    # save loss curves\n",
    "    linestyles = ['solid', 'dashed']\n",
    "    i = 0\n",
    "    for dataset_name in ['train', 'val']:\n",
    "        plt.plot(logs[dataset_name]['loss'][:], linestyle=linestyles[i],\n",
    "                label=dataset_name)\n",
    "        i += 1\n",
    "    if e == 1:\n",
    "        plt.legend()\n",
    "    plt.xlabel('e')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig('%s/loss_curves.pdf' % (result_path), dpi=600)\n",
    "        \n",
    "    # update models and logs if the best validation loss is updated\n",
    "    current_val_loss = logs['val']['loss'][-1]\n",
    "    best_val_loss = current_val_loss if e == 1 else np.minimum(best_val_loss, current_val_loss)\n",
    "    if best_val_loss == current_val_loss:\n",
    "        # update model and logs\n",
    "        best_val_e = e\n",
    "        os.makedirs('%s/' % result_path, exist_ok=True)\n",
    "        torch.save({'prior': prior,\n",
    "                    'encoder': encoder,\n",
    "                    'decoder': decoder,\n",
    "                    'logs': logs,\n",
    "                    'num_e': num_e,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_worker': num_worker,\n",
    "                    'seed': seed,\n",
    "                    'beta': beta,\n",
    "                    'Adam_beta1': Adam_beta1,\n",
    "                    'Adam_beta2': Adam_beta2,\n",
    "                    'weight_decay': weight_decay,\n",
    "                    'init_lr': init_lr,\n",
    "                    'lr_milestones': lr_milestones,\n",
    "                    'lr_gamma': lr_gamma,\n",
    "                    'dtype': dtype,\n",
    "                    'M': M,\n",
    "                    'alpha_step': alpha_step,\n",
    "                    'fix_alpha': fix_alpha,\n",
    "                    'result_path': result_path},\n",
    "                    '%s/model.pth' % result_path)\n",
    "    if e == num_e:\n",
    "        # update logs\n",
    "        saved_model = torch.load('%s/model.pth' % result_path)\n",
    "        saved_model['logs'] = logs\n",
    "        torch.save(saved_model, '%s/model.pth' % result_path)\n",
    "\n",
    "    current_summary_stats_row = {}\n",
    "\n",
    "    current_summary_stats_row['e'] = e\n",
    "    current_summary_stats_row['best_val_e'] = best_val_e\n",
    "    current_summary_stats_row['train_loss'] = logs['train']['loss'][-1]\n",
    "    current_summary_stats_row['val_loss'] = logs['val']['loss'][-1]\n",
    "    current_summary_stats_row['train_recon_loss_post'] = logs['train']['recon_loss_post'][-1]\n",
    "    current_summary_stats_row['val_recon_loss_post'] = logs['val']['recon_loss_post'][-1]\n",
    "    current_summary_stats_row['train_kl_post_prior'] = logs['train']['kl_post_prior'][-1]\n",
    "    current_summary_stats_row['val_kl_post_prior'] = logs['val']['kl_post_prior'][-1]\n",
    "    current_summary_stats_row['train_recon_loss_encoded'] = logs['train']['recon_loss_encoded'][-1]\n",
    "    current_summary_stats_row['val_recon_loss_encoded'] = logs['val']['recon_loss_encoded'][-1]\n",
    "    current_summary_stats_row['train_kl_encoded_prior'] = logs['train']['kl_encoded_prior'][-1]\n",
    "    current_summary_stats_row['val_kl_encoded_prior'] = logs['val']['kl_encoded_prior'][-1]\n",
    "    current_summary_stats_row['l2_penalty'] = logs['train']['l2_penalty'][-1]\n",
    "        \n",
    "    summary_stats.append(current_summary_stats_row)\n",
    "    pd.DataFrame(summary_stats).to_csv('%s/summary_stats.csv' % result_path, index=False)\n",
    "    \n",
    "    \n",
    "print(\"Done training Conv-CI-iVAE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cac36-46f5-4f3d-8a37-f5792edf39fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
